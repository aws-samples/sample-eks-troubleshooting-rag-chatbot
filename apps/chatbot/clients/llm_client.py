import boto3
import json
import requests
import os
from utils.logger import logger


def encode_query(query):
    """
    Generates an embedding for the provided query using Amazon Bedrock's embedding model.

    Parameters:
        query (str): The input text query to generate an embedding.

    Returns:
        list: The embedding generated by the Bedrock model.
    """
    # Initialize Bedrock client
    bedrock_runtime = boto3.client(
        service_name='bedrock-runtime'
    )

    # Call Bedrock to generate embedding
    response = bedrock_runtime.invoke_model(
        modelId="amazon.titan-embed-text-v2:0",
        contentType="application/json",
        accept="application/json",
        body=json.dumps({"inputText": query})
    )

    # Extract embedding from response
    embedding = json.loads(response.get('body').read())['embedding']

    return embedding


def invoke_claude(prompt_text):
    """
    Sends a prompt to the Claude model via Amazon Bedrock and returns the model's response.

    Parameters:
        prompt_text (str): The input prompt to be sent to the Claude model.

    Returns:
        str: The text content of the response generated by Claude.
    """
    # Initialize the Bedrock client
    bedrock_client = boto3.client(service_name='bedrock-runtime')

    # Define the request body for the Claude model
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1000,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text
                    }
                ]
            }
        ]
    }

    # Invoke the Claude model through the Bedrock API
    response = bedrock_client.invoke_model(
        modelId='anthropic.claude-3-sonnet-20240229-v1:0',
        contentType='application/json',
        accept='application/json',
        body=json.dumps(body)
    )

    # Parse the model's response
    response_body = json.loads(response['body'].read())
    response_text = response_body['content'][0]['text']

    return response_text


def invoke_deepseek_vllm(prompt_text):
    """
    Sends a prompt to the DeepSeek model hosted with vLLM via a POST request and returns the model's response.

    Parameters:
        prompt_text (str): The input prompt to be sent to the DeepSeek model.

    Returns:
        str: The response content from the DeepSeek model, or an error message if the request fails.
    """
    url = os.getenv("VLLM_ENDPOINT", "http://deepseek-gpu-vllm-chart.deepseek.svc.cluster.local:80")
    url_complete = f"{url}/v1/chat/completions"

    headers = {
        "Content-Type": "application/json"
    }

    payload = {
        "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "messages": [
            {
                "role": "user",
                "content": prompt_text
            }
        ]
    }

    try:
        response = requests.post(url_complete, headers=headers, json=payload)

        # Log detailed debug information
        logger.debug(f"Request URL: {url_complete}")
        logger.debug(f"Request Headers: {headers}")
        logger.debug(f"Request Payload: {json.dumps(payload, indent=2)}")
        logger.debug(f"Response Status Code: {response.status_code}")
        logger.debug(f"Response Headers: {dict(response.headers)}")

        try:
            logger.info(f"Response Body: {response.text}")
        except:
            logger.error("Could not print response body")

        response.raise_for_status()

        result = response.json()

        if "choices" in result and len(result["choices"]) > 0:
            return result["choices"][0]["message"]["content"]
        else:
            return "No response content found"

    except requests.exceptions.RequestException as e:
        error_msg = f"Error making request to vLLM: {str(e)}"
        if hasattr(e.response, 'text'):
            error_msg += f"\nResponse body: {e.response.text}"
        logger.error(error_msg)
        return f"Error: {str(e)}"
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON response: {str(e)}")
        return f"Error decoding response: {str(e)}"
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        return f"Unexpected error: {str(e)}"


def construct_prompt(query, retrieved_docs):
    """
    Constructs a prompt for the model, including a user query and relevant context.

    Parameters:
        query (str): The user's query to be included in the prompt.
        retrieved_docs (list): A list of relevant documents or context to include in the prompt.

    Returns:
        str: The constructed prompt, including the user query and any relevant context.
    """
    if not retrieved_docs:
        context = "No relevant logs found."
    else:
        context = "\n".join(retrieved_docs)

    kubectl_prompt = "When needed Generate a kubectl command to get more details about the relevant logs, use a key 'KUBECTL_COMMAND: command' if true for to parse, make sure that you have real pod names not templates"
    return f"Instructions: {kubectl_prompt} \n\nUser Query: {query} \n\nContext:\n{context}\n\nResponse:"
